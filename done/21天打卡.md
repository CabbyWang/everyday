```
#21天打卡#Day1
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月13日
学习要点&总结:
数组
1. 数组(Array)是一种线性表数据结构。它由一组连续的内存空间，来存储一组具有相同类型的数据。
2. 随机访问时间复杂度O(1), 插入/删除/查找 时间复杂度O(n), 排好序的数组使用二分查找时间复杂度O(logn)。
3. 警惕数组越界问题。
4. 容器和数组。容器将很多数组操作的细节封装起来。对于业务开发，直接使用容器就足够了，省时省力。
5. 删除时可以先不删除，只是做个标记，等到数组容量快达到阈值时，统一删除，节省数据搬移的成本。
```

```
#21天打卡#Day2
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月14日
学习要点&总结:
链表
1. 数组需要一块连续的内存空间来存储，而链表不需要，链表通过"指针"将一组零散的内存块串联起来使用。
2. 单链表
 每个链表的结点存储数据和记录下一个结点地址的指针(后继指针next)。
 时间复杂度: 
   插入/删除 理论上O(1) 实际O(n) 找到前驱结点还是需要遍历链表，时间复杂度O(n)
   查找 O(n)  因为链表中的数据不是连续的，不能通过首地址和下标来寻址，只能遍历链表
3. 循环链表
 单链表的尾指针指向空地址，循环链表的尾指针指向头结点。
 约瑟夫问题
4. 双向链表
 拥有 前驱指针prev 后继指针next.
 时间复杂度:
   删除给定值的结点 O(n)
   删除给指针指向的结点 O(1)
   插入 O(1)
5. 回文字符串问题
6. 缓存淘汰策略
 FIFO(First In First Out),先进先出策略
 LFU(Least Frequently Used),最少使用策略
 LRU(Least Recently Used),最近最少使用策略
 维护一个有序单链表, 越靠近尾部的结点是越早访问的，当有新数据被访问时，从链表开头顺序遍历链表。如果数据已经存在，则删除，然后插入到链表头部；如果数据不存在，则直接存入到链表头部，如果此时缓存满了则删除尾结点后再插入。
 优化思路，引入散列表记录每个数据的位置，将访问缓存的时间复杂度降到O(1)
```

```
#21天打卡#Day3
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月15日
学习要点&总结:
链表(下)
1. 指针和引用: 存储所指对象的内存地址。
	将某个变量赋值给指针，实际上就是将变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。
2. 警惕指针丢失和内存泄漏
3. 利用哨兵简化实现难度
4. 留意边界条件处理
	a. 链表为空
	b. 链表只包含一个结点
	c. 链表只包含两个结点
	d. 代码逻辑在处理头结点和尾结点时，是否能正常工作
5. 举例画图，辅助思考
	a. 举例法
	b. 画图法
```

```
#21天打卡#Day4
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月16日
学习要点&总结:
栈
1. “操作受限“的线性表。后进者先出，先进者后出。
2. 当某个数据集合只涉及在一端插入和删除数据，并且满足后进后出、先进先出的特性，就应该首选”栈“这种数据结构。
3. 顺序栈: 用数组实现的栈。
   链式栈: 用链表实现的栈。
4. 操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数调用时的临时变量。
5. 表达式求值
6. 括号匹配
7. 浏览器的前进后退功能
```

```
#21天打卡#Day5
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月17日
学习要点&总结:
队列
1. “操作受限“的线性表。先进者先出，后进者后出。
3. 顺序队列: 用数组实现的队列。
   链式队列: 用链表实现的队列。
4. 循环队列
	解决数据搬移的问题。
	写出没有bug的循环队列的关键是，确定好队空和队满的判定条件。tail=head时队空， (tail+1)/n=head时队满，此时会浪费一个数组的存储空间。
5. 阻塞队列和并发队列
	阻塞队列: 在队列的基础上增加了阻塞操作。队列为空时，从队头取数据会被阻塞；队列已经满时，插入数据会被阻塞。
	并发队列: 线程安全的队列叫做并发队列。 基于数组的循环队列，利用CAS(Compare and Swap)原子操作，可以实现非常高效的并发队列。
6. 考虑使用CAS实现无锁队列，则在入队前，获取tail位置，入队时比较tail是否发生变化，如果否，则允许入队，反之，本次入队失败。出兑则是获取head的位置，进行cas。
```

```
#21天打卡#Day6
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月18日
学习要点&总结:
递归
1. 递归需要满足的三个条件
	1. 一个问题的解可以分解为几个子问题的解
	2. 这个问题与分解之后的子问题，出了数据规模不同，求解思路完全一样
	3. 存在递归终止条件
2. 如何编写递归代码: 写出递推公式，找到终止条件
	写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。
	只要遇到递归，就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。
3. 递归代码要警惕堆栈溢出
	在代码中限制递归调用的最大深度
4. 递归代码要警惕重复计算
```

```
#21天打卡#Day7
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月19日
学习要点&总结:
排序(上)
1. 排序算法的执行效率
	1. 最好情况、最坏情况、平均情况时间复杂度
	2. 时间复杂度的系数、常数、低阶
	3. 比较次数和交换(或移动)次数
2. 排序算法的内存消耗(空间复杂度?)
3. 排序算法的稳定性
	稳定的排序算法: 排序前后相等的元素前后顺序没有变化
	不稳定的排序算法
4. 冒泡排序
	遍历n次， 每次将两个元素比较，顺序错误就交换
	1. 原地排序算法
	2. 是稳定的排序算法
	3. 时间复杂度: O(n2)
5. 选择排序
	每次在为排序序列中找到最小元素，存放到已排序序列的尾部
	1. 原地排序算法
	2. 不是稳定的排序算法
	3. 时间复杂度: O(n2)
6. 插入排序
	在未排序序列中取出第一个元素，与已排序序列一个个比较，然后找到相应位置并插入
	1. 原地排序算法
	2. 是稳定的排序算法
	3. 时间复杂度: O(n2)
```

```
#21天打卡#Day8
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月20日
学习要点&总结:
排序(下)
1. 归并排序
	排序一个数组，先将数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组都有序了。
	1. 是稳定的排序算法
	2. 时间复杂度: O(nlogn)
	3. 空间复杂度: O(n)
2. 快速排序
	在数组中选择一个pivot(分区点)，遍历数组将小于pivot的放到左边，大于pivot的放到右边。然后按照同样的操作处理两个分区。
	1. 不是稳定的排序算法
	2. 时间复杂度: O(nlogn) 最坏情况时间复杂度O(n2)
	3. 空间复杂度: O(1)
```

```
#21天打卡#Day9
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月21日
学习要点&总结:
线性排序
1. 桶排序
	1. 将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序取出，组成的序列就是有序的了。
	2. 当桶的个数接近数据个数时，时间复杂度接近O(n)。
	最坏情况下，数据集中在一个桶里，时间复杂度退化为O(nlongn)。
	3. 桶排序比较适合用在外部排序中(数据存储在外部磁盘中，数据量比较大，内存有限，不能全部加载到内存中进行排序)。
2. 计数排序
	1. 计数排序其实是桶排序的一种特殊情况。
	2. 只能用在数据范围不大的场景中。
	3. 计数排序只能给非负整数排序，如果要排序的数据是其他类型，要将其在不改变相对大小的情况下，转化为非负整数。
3. 基数排序
	1. 基数排序对要排序的数据是有要求的，需要可以分割出独立的"位"来比较，而且位之间有递进的关系，如果a数据的高位比b数据大，那剩下的地位就不用比较了。
	2. 每一位的数据范围不能太大。
	3. 每一位要用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到O(n)了。
```

```
#21天打卡#Day10
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月22日
学习要点&总结:
排序优化:如何实现一个通用的、高性能的排序函数。
1. 线性排序算法的时间复杂度比较低，使用场景比较特殊；小规模数据，可以选择O(n2)的算法；如果对大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效。
2. 如何优化快速排序?
	1. 三数取中法(五数取中、十数取中)
		从首、尾、中间，分别取出一个数，然后对比大小，取中间值作为分区点。
	2. 随机法
		从要排序的区间中，随机选择一个元素作为分区点。
3. 防止堆栈溢出
	1. 限制递归深度
	2. 在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有系统栈大小的限制。
4. c语言的qsort排序函数
	1. 小数据量使用归并排序
	2. 大数据量使用快排
	3. 快排选择分区点的方法是"三数取中法"
	4. 在快排的过程中，当排序的区间元素小于等于4时，退化为插入排序。
```

```
#21天打卡#Day11
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月23日
学习要点&总结:
二分查找(上):如何用最省内存的方式实现快速查找功能
1. 二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0.
2. O(logn)的惊人查找速度
3. 非递归实现
	1. 循环退出条件
	2. mid的取值
	3. low和high的更新
4. 递归实现
5. 二分查找应用场景的局限性:
	1. 二分查找依赖的是顺序表结构(数组)
	2. 二分查找针对的是有序数据
	3. 数据量太小不适合二分查找
		顺序遍历就足够了
	4. 数据量太大不适合二分查找
		需要"连续"的内存空间
6. 大部分情况下，能用二分查找解决的，都可以使用散列表和二叉树来解决。
```

```
#21天打卡#Day12
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月24日
学习要点&总结:
二分查找(下):如何快速定位IP对应的省份地址？
1. 4种常见的二分查找变形问题
	查找第一个值等于给定值的元素
	查找最后一个值等于给定值的元素
	查找第一个大于等于给定值的元素
	查找最后一个小于等于给定值的元素
2. 如果快速定位IP对应的省份地址
	可以转化为"查找最后一个小雨等于给定值的元素"的问题。找到最后一个起始IP小于等于这个IP的IP区间，然后检查这个IP是否在这个IP区间内，如果在，就取出对应的归属地显示；如果不在，就返回未查找到。
3. 在"近似"查找问题上，二分查找的优势更加明显。散列表和二叉树，比较难以实现。
```

```
#21天打卡#Day13
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月25日
学习要点&总结:
跳表: 为什么Redis一定要用跳表来实现有序集合？
1. 每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。
2. 链表加多级索引的结构，就是跳表。
3. 空间复杂度: O(n)
4. 查找/插入/删除操作时间复杂度都为O(logn)
5. 红黑树、AVL树这样的平衡二叉树是通过左右旋的方式保持左右子树的大小平衡；
	跳表是通过随机函数来维护平衡性。
6. Redis的有序集合通过跳表来实现
	1. 插入、删除、查找时间复杂度和红黑树相同。
	2. 按照区间来查找数据的操作，红黑树的效率没有跳表高。
	3. 跳表的代码实现比红黑树简单。可读性好，不容易出错。
	4. 跳表更加灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。
7. 红黑树比跳表的出现要早一些，很多编程语言的Map类型都是通过红黑树来实现的。
```

```
#21天打卡#Day14
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月26日
学习要点&总结:
散列表(上):Word文档的单词拼写检查功能是如何实现的
1. 散列表其实是数组的一种扩展，由数组演化而来 
	散列表用的是数组支持按照下表随机访问数据的特性
2. key -> hash function -> table
	key通过散列函数计算出一个非负整数作为数组下标，value存储在数组的对应位置上
3. 散列函数设计的基本要求:
	1. 散列函数计算得到的散列值是一个非负整数
	2. 如果 key1 = key2，则 hash(key1) = hash(key2)
	3. 如果 key1 != key2, 则 hash(key1) != hash(key2)
4. 散列冲突
	1. 开放寻址法
		出现散列冲突时，就重新探测一个空闲位置，将其插入
		1. 线性探测 (每次探测步长1)
		2. 二次探测 (每次探测步长变为之前的"二次方")
		3. 双重散列 (使用一组散列函数)
	2. 链表法
```

```
#21天打卡#Day15
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月27日
学习要点&总结:
散列表(中):如何打造一个工业级水平的散列表?
1. 支持快速的查询、插入、删除操作(设计一个合适的散列函数)
  1. 散列碰撞攻击？
  2. 散列函数的设计不能太复杂(减少计算时间)
     散列函数生成的值要尽可能随机并且均匀分布(最小化冲突)
  3. 散列函数设计方法：
    1. 数据分析法
    2. 直接寻址法
    3. 平方取中法
    4. 折叠法
    5. 随机数法
    6. ...
2. 内存占用合理，不能浪费过多的内存空间(定义装载因子阈值，设计动态扩容策略)
  1. 动态扩容/动态缩容(设置装载因子阈值)
    装载因子过大时扩容，需要通过散列函数重新计算每个数据的存储位置
  2. 解决一次性扩容耗时过多的情况
    将扩容操作穿插在插入操作的过程中，分批完成。当装载因子达到阈值后，只申请新空间，但不将老的数据搬移到新散列表中；当有新数据插入时，将老散列表中拿出一个数据放入新散列表。
3. 性能稳定，极端情况下，性能也不会退化到无法接受的情况(选择合适的散列冲突解决方法)
  1. 开放寻址法
    优点: 对CPU缓存友好
         方便序列化
    缺点: 删除数据比较麻烦(需要特殊标记已经删除的数据)
         冲突代价更高 更浪费内存空间
    总结: 当数据量比较小、装载因子小的时候，适合开放寻址法
  2. 链表法
    优点: 内存利用率高
         对大装载因子的容忍度更高
    缺点: 对CPU缓存不友好
    总结: 链表法适合存储大对象、大数据量的散列表。比起开放寻址法更加灵活，支持更多优化策略，比如用红黑树、跳表代替链表。
```

```
#21天打卡#Day16
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月28日
学习要点&总结:
散列表(下):为什么散列表和链表经常会一起使用?
1. LRU缓存淘汰算法
	单纯使用链表实现LRU的时间复杂度很高，是O(n)
	将链表和散列表组合使用，查询操作变为O(1), 进而每次操作都为O(1)
	使用双向链表，前驱和后继指针将结点串在双向链表中，hnext指针将结点串在散列表的拉链中。
2. Redis有序集合
	我们不仅会通过score来查找数据，还会通过key来查找数据
	按照分值将成员对象组织成跳表结构，按照键值构建一个散列表
3. Java LinkedHashMap
	HashMap是通过散列表实现的，LinkedHashMap多了一个Link(双向链表)
	不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据(和LRU缓存淘汰策略原理一样), 一个支持LRU缓存淘汰策略的缓存系统
```

```
#21天打卡#Day17
打卡专栏: 数据结构与算法之美
打卡时间: 2020年2月29日
学习要点&总结:
哈希算法(上):如何防止数据库中的用户信息被脱库？
1. 将任何长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法。
	通过原始数据映射之后得到的二进制值串就是哈希值。
2. 一个优秀的哈希算法需要满足的要求:
	1. 从哈希值不能反向推导出原始数据
	2. 对输入数据非常敏感。哪怕原始数据只修改了一个bit，生成的哈希值也大不相同
	3. 散列冲突概率要小
	4. 哈希算法的执行效率要尽量高效
3. 哈希算法的应用:
	1. 安全加密
	2. 唯一标志
	3. 数据校验
	4. 散列函数
4. 区块链:
	每个区块由区块头和区块体组成
	区块头保存着 自己区块体 和 上一个区块头 的哈希值
	使用的SHA256哈希算法
```

```
#21天打卡#Day18
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月1日
学习要点&总结:
哈希算法(下):哈希算法在分布式系统中有哪些应用？
1. 负载均衡
	利用哈希算法代替映射表，可以实现一个胡话粘滞的负载均行策略。
2. 数据分片
	通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。
3. 分布式存储
	利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致的数据大量搬移的难题。
```

```
#21天打卡#Day19
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月2日
学习要点&总结:
二叉树基础(上):什么样的二叉树适合用数组来存储？
1. 树(Tree)
	根节点
	叶子节点
	父节点
	子节点
	兄弟节点
	节点的高度=节点到叶子结点的最长路径(边数)
	节点的深度=节点到根节点的边数
	节点的层数=节点的深度 + 1
	树的高度=根节点的高度
2. 二叉树(Binary Tree)
	满二叉树: 叶子节点全在最底层，除了叶子节点之外，每个节点都有左右两个子节点。
	完全二叉树: 叶子结点都在最底下两层，最后一层的叶子结点都靠左排列，除了最后一层，其他层的结点个数都达到最大。
3. 链式存储法
	 顺序存储法
	 完全二叉树用数组存储是最节省内存的方式
	 堆是完全二叉树，最常用的存储方式是数组
4. 二叉树的遍历
	二叉树的前、中、后序遍历就是个递归的过程
	前序遍历: 节点-左-右
	中序遍历: 左-节点-右
	后序遍历: 左-右-节点
```

```
#21天打卡#Day20
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月3日
学习要点&总结:
二叉树基础(下):有了如此高效的散列表，为什么还需要二叉树？
1. 二叉查找树(binary Search Tree)
	在树中的任意一个节点，左子树中每个节点的值，都小于这个节点的值；右子树中每个节点的值，都大于这个节点的值
2. 查找
	最坏情况下退化为链表，时间复杂度O(n)
	最好情况下为完全二叉树(满二叉树)，时间复杂度O(logn)
3. 插入
	1. 插入数据大于节点的数据，并且右子树为空，就将数据插到右子节点；如果不为空，递归右子树插入
	2. 插入数据小于节点的数据，并且左子树为空，就将数据插到左子节点；如果不为空，递归左子树插入
	3. 插入相同数据
		a. 通过链表或支持动态扩容的数组等数据结构，将相同的数据存储在同一个节点上
		b. 将相同数据插入到右子树，把插入的数据当作大于节点的值来处理。相应需要改造一下查找和删除。
4. 删除
	a. 删除叶子节点
	b. 删除的节点只有一个子节点
	c. 删除的节点右两个子节点
```

```
#21天打卡#Day21
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月4日
学习要点&总结:
红黑树(上):为什么工程中都用红黑树这种二叉树？
1. 平衡二叉树
	严格定义: 二叉树中任意一个节点的左右子树的高度相差不能大于1.
2. Treap(树堆)、Splay Tree(伸展树)
	绝大多数情况下，操作效率都很高，但是无法避免极端情况下时间复杂度的退化。不适合对于单次操作时间非常敏感的场景。
2. AVL树
	严格符合平衡二叉树的定义
	每次插入、删除都要调整，比较复杂、耗时。不适合右频繁插入删除操作的数据集合
3. 红黑树
	近似平衡二叉树
	满足以下要求:
		a. 根节点是黑色的
		b. 每个叶子结点都是黑色的空节点
		c. 任何响铃的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的
		d. 每个节点，从该节点到达其可达叶子结点的所有路径，都包含相同数目的黑色节点
```

```
#21天打卡#Day22
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月5日
学习要点&总结:
红黑树(下)
1. 红黑树的平衡跟魔方复原神似
	遇到什么样的节点排布，我们就对应怎么去调整
2. 调整操作
	左旋:围绕某个节点的左旋
	右旋
	改变颜色
3. 红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。
4. 总结
	a. 把红黑树的平衡调整的过程比做魔方复原，不要过于深究这个算法的正确性
	b. 找准关注节点，不要搞丢、搞错关注节点
	c. 插入操作的平衡调整比较简单，删除操作比较复杂
```

```
#21天打卡#Day23
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月6日
学习要点&总结:
递归树:如何借助树来求解递归算法的时间复杂度?
1. 将大问题分解成小问题这一层层的分解过程画成图，会成为一棵树，这棵树叫递归树
2. 分析快速排序的时间复杂度
3. 分析斐波那契数列的时间复杂度
4. 分析全排列的时间复杂度
```

```
#21天打卡#Day24
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月7日
学习要点&总结:
堆和堆排序:为什么说堆排序没有快速排序块?
1. 定义:
	堆是一个完全二叉树
	堆中每一个节点的值都必须大于等于(或小于等于)其子树中每个节点的值
2. 往堆中插入一个元素
	堆化(heapify) 从下往上堆化
3. 删除堆顶元素
	把最后一个节点放到堆顶，然后从上往下堆化
4. 建堆的时间复杂度 O(n)
	 堆化的时间复杂度 O(logn)
5. 如何基于堆实现排序
	1. 建堆
	2. 排序
6. 堆排序相对于快速排序，在排序过程中，交换次数要多余快排
```

```
#21天打卡#Day25
打卡专栏: 数据结构与算法之美
打卡时间: 2020年3月8日
学习要点&总结:
堆的应用:如何快速获取到Top10最热门的搜索关键词?
1. 优先级队列
	a. 合并有序小文件
	b. 高性能定时器
2. 利用堆求Top K
3. 利用堆求中位数
	大顶堆
	小顶堆
4. 百分比响应时间(类似求中位数)
```

